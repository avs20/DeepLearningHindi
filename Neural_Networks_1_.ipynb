{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Networks -1 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avs20/DeepLearningHindi/blob/main/Neural_Networks_1_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e74fnyNIoYf"
      },
      "source": [
        "What basically happnes at each neuron?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvL4UIaKIGow",
        "outputId": "24e214bd-fc3c-4b4a-8b80-925ad9bf5dd4"
      },
      "source": [
        "inputs = [1, 2, 3]\n",
        "weights = [0.2, 0.8, -0.5]\n",
        "bias = 2 \n",
        "\n",
        "output = ( inputs[0]*weights[0] + \n",
        "           inputs[1]*weights[1] + \n",
        "           inputs[2]*weights[2] + bias )\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0L4BGNIxif"
      },
      "source": [
        "What if we have 4 inputs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxhROePckKwJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBdiGDa0Il1M",
        "outputId": "05e9737d-aa0c-4577-9c38-0bd230b97c95"
      },
      "source": [
        "inputs = [1, 2, 3, 2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2 \n",
        "\n",
        "output = ( inputs[0]*weights[0] + \n",
        "           inputs[1]*weights[1] + \n",
        "           inputs[2]*weights[2] + \n",
        "           inputs[3]*weights[3] + bias )\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGkKBqzJJKHO"
      },
      "source": [
        "Layers have more than 1 neurons.\n",
        "Each neuron in the neuron takes the same input, input given to the layer. \n",
        "But each neuron has it's own weight and bias\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hyZssrrI8gc",
        "outputId": "ea7018f1-6fd2-4807-a161-05c6516b15c7"
      },
      "source": [
        "inputs = [1, 2, 3, 2.5]\n",
        "weights0 = [0.2, 0.8, -0.5, 1.0]\n",
        "weights1 = [0.4,0.8,-0.6,-0.7]\n",
        "weights2 = [0.9, 0.4, 0.8, 0.9]\n",
        "\n",
        "bias1 = 2.0\n",
        "bias2 = -1.0\n",
        "bias3 = 0\n",
        "\n",
        "output = [ inputs[0] * weights0[0] + inputs[1] * weights0[1] + inputs[2] * weights0[2] + inputs[3]* weights0[3] + bias1 ,\n",
        "           inputs[0] * weights1[0] + inputs[1] * weights1[1] + inputs[2] * weights1[2] + inputs[3]* weights1[3] + bias2 ,\n",
        "           inputs[0] * weights2[0] + inputs[1] * weights2[1] + inputs[2] * weights2[2] + inputs[3]* weights2[3] + bias3 ]\n",
        "    \n",
        "print(output)\n",
        "          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8, -2.55, 6.3500000000000005]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9oDIaLrKdRF",
        "outputId": "fef30ef8-c684-41c3-a0bf-06e0c3202587"
      },
      "source": [
        "inputs = [1, 2, 3, 2.5]\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.4,0.8,-0.6,-0.7],\n",
        "           [0.9, 0.4, 0.8, 0.9]]\n",
        "biases = [2.0, -1.0, 0]\n",
        "\n",
        "layer_outputs = []\n",
        "\n",
        "for neuron_weights, bias in zip(weights, biases):\n",
        "  output = 0\n",
        "\n",
        "  for input, weight in zip(inputs, neuron_weights):\n",
        "    output += input * weight\n",
        "  \n",
        "  # add bias \n",
        "  output += bias\n",
        "\n",
        "  # add to final \n",
        "  layer_outputs.append(output)\n",
        "\n",
        "print(layer_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8, -2.55, 6.3500000000000005]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCyeFkucUsSO"
      },
      "source": [
        "Vectors and Dot product"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY6a05loLQnX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2564ba1-4134-4ba2-aec8-eccb426d371a"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "inputs = [1, 2, 3]\n",
        "weights = [0.2, 0.8, -0.5]\n",
        "bias = 2 \n",
        "\n",
        "output = np.dot(inputs, weights) + bias\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrraL0QSVUQE"
      },
      "source": [
        "Layer of neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8c6IM3WU4Fl",
        "outputId": "97faf19b-2ef6-43d7-b534-805b6e03b843"
      },
      "source": [
        "inputs = [1, 2, 3, 2.5]\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.4,0.8,-0.6,-0.7],\n",
        "           [0.9, 0.4, 0.8, 0.9]]\n",
        "biases = [2.0, -1.0, 0]\n",
        "\n",
        "layer_outputs = np.dot(weights, inputs) +biases\n",
        "\n",
        "\n",
        "\n",
        "print(layer_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4.8  -2.55  6.35]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "iyORlXIrVaUq",
        "outputId": "ebc8f7fe-502b-4226-873a-80fb92f28172"
      },
      "source": [
        "inputs = [1, 2, 3, 2.5]\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.4,0.8,-0.6,-0.7],\n",
        "           [0.9, 0.4, 0.8, 0.9]]\n",
        "biases = [2.0, -1.0, 0]\n",
        "\n",
        "layer_outputs = np.dot(inputs, weights) +biases\n",
        "\n",
        "\n",
        "\n",
        "print(layer_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-30a4b0238793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (4,) and (3,4) not aligned: 4 (dim 0) != 3 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgZhz87FngIG",
        "outputId": "8c0e5c3c-621a-460a-fd41-d3be59fd4b11"
      },
      "source": [
        "import numpy as np \n",
        "inputs = [1, 2, 3]\n",
        "weights = [[0.2, 0.8, -0.5 ],\n",
        "           [0.4,0.8,-0.6],\n",
        "           [0.9, 0.4, 0.8]]\n",
        "biases = [2.0, -1.0, 0]\n",
        "\n",
        "layer_outputs = np.dot(inputs, weights) +biases\n",
        "\n",
        "\n",
        "\n",
        "print(layer_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.7 2.6 0.7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2yAVEmdXEA_"
      },
      "source": [
        "matrix multiplication is dot product of combination of rows from the first matrix and columns from the second matrix. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeCYG-z9Wr9Q"
      },
      "source": [
        "Data is often comes in input batche. \n",
        "\n",
        "What is a batch? a group of input data at a time. \n",
        "\n",
        "Now we need to multiply this group of input data with group of weights both are matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A46Sa9yrVkbz",
        "outputId": "10d64988-89fe-46cf-acdb-551ed143e026"
      },
      "source": [
        "a = np.array([1,2,3])\n",
        "b = np.array([4,5,6])\n",
        "\n",
        "output = np.dot(a,b)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34MYLcCW9kwR"
      },
      "source": [
        "Batch of data with layer of neurons\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-fMTywLW0nd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b51d149-32ae-4664-caae-61997ae8a3da"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
        "          [2.0,5.0,-1.0,2.0],\n",
        "          [-1/5, 2.7, 3.3, -0.8]]\n",
        "      \n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases  = [2.0, 3.0, 0.5]\n",
        "\n",
        "layer_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
        "\n",
        "print(layer_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.67   1.701 -0.312]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIp1P2tXJgu4"
      },
      "source": [
        "## More hidden layers and non-linear data\n",
        "\n",
        "A deep neural network is a neural network with 2 or more hidden layers\n",
        "\n",
        "So for each layer we will have a different set of weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88MZDkRZ-Pfl"
      },
      "source": [
        "import numpy as np \n",
        "inputs = [1, 2, 3]\n",
        "weights = [[0.2, 0.8, -0.5 ],\n",
        "           [0.4,0.8,-0.6],\n",
        "           [0.9, 0.4, 0.8]]\n",
        "biases = [2.0, -1.0, 0]\n",
        "\n",
        "weights2 = [[ 0.1, -0.14, 0.5],\n",
        "            [-0.5, 0.12, -0.33],\n",
        "            [-0.44, 0.73, -0.13]]\n",
        "\n",
        "biases2 = [-1,2, -0.5]\n",
        "\n",
        "layer_outputs = np.dot(inputs, weights) +biases\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWRQ-FjsKHHJ"
      },
      "source": [
        "###Q how many neurons we have in the second layer?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtAwEp2uKK-B",
        "outputId": "cdde02f8-6a59-4447-cf73-fa0bb0b34630"
      },
      "source": [
        "import numpy as np \n",
        "inputs = [[1.0, 2.0, 3.0, 2.5],\n",
        "          [2.0,5.0,-1.0,2.0],\n",
        "          [-1/5, 2.7, 3.3, -0.8]]\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "biases = [2.0, -1.0, 0]\n",
        "\n",
        "weights2 = [[ 0.1, -0.14, 0.5],\n",
        "            [-0.5, 0.12, -0.33],\n",
        "            [-0.44, 0.73, -0.13]]\n",
        "\n",
        "biases2 = [-1, 2, -0.5]\n",
        "\n",
        "layer1_outputs = np.dot(inputs, np.array(weights).T) +biases\n",
        "\n",
        "\n",
        "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
        "\n",
        "print(layer2_outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.8131  -1.35685 -4.89375]\n",
            " [ 0.5534  -3.0482  -8.6183 ]\n",
            " [-0.91714  1.15708 -2.80751]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZrI_VM7KYHk",
        "outputId": "8f9b114e-5cc6-4d3a-9ea0-ca51127dc367"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnfs\n",
            "  Downloading https://files.pythonhosted.org/packages/06/8c/3003a41d5229e65da792331b060dcad8100a0a5b9760f8c2074cde864148/nnfs-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9zr04ZsLDj5"
      },
      "source": [
        "import nnfs\n",
        "nnfs.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "6_5HFYddLK3U",
        "outputId": "e6514424-73c4-4baa-e16a-6fb97cf4e168"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nnfs.datasets import spiral_data\n",
        "X,y = spiral_data(samples=100, classes=3)\n",
        "plt.scatter(X[:,0], X[:,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "lnO35ahILaUB",
        "outputId": "250817a6-c3df-4bbd-bb29-1314919e75b1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nnfs.datasets import spiral_data\n",
        "X,y = spiral_data(samples=100, classes=3)\n",
        "plt.scatter(X[:,0], X[:,1], c = y, cmap='brg')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocLIqmziL4VI"
      },
      "source": [
        "## Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65n7_RRpLw0S",
        "outputId": "4631f07e-a622-4d1c-f568-f64ba09fe00c"
      },
      "source": [
        "import numpy as np \n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "#Dense Layer\n",
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    #initialize weight and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1,n_neurons))\n",
        "\n",
        "  # forwards pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# test class\n",
        "\n",
        "X,y = spiral_data(samples = 100, classes=3)\n",
        "\n",
        "#create the layer\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "\n",
        "#perform a forward pass\n",
        "dense1.forward(X)\n",
        "\n",
        "# see output\n",
        "print(dense1.output[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
            " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
            " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
            " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
            " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4ElCfFS-o3U"
      },
      "source": [
        "### Activation Functions\n",
        "\n",
        "Activation function is applied to the output of a neuron, which modifies outputs.\n",
        "\n",
        "We use activation function because is activation function is non-linear, it allows for neural networks with 2 or more layers to map non-linear functions.\n",
        "\n",
        "There are generally 2 types of activation functions used in NN. \n",
        "One in the hidden layers and 1 in the final output layer. \n",
        "\n",
        "How non-linearity comes will see later. Generally there are following types \n",
        "\n",
        "1. Step activation function \n",
        "2. Linear activation function (Last Layer for regression)\n",
        "3. Sigmoid activation function\n",
        "4. Rectified Linear Units \n",
        "\n",
        "\n",
        "Refs\n",
        "----\n",
        "1. https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrHcBtZAZ20y"
      },
      "source": [
        "ReLU Activation : simple code \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GDM6Zz4altw",
        "outputId": "f39c801e-7386-40c0-cb1a-82178b390a0e"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnfs\n",
            "  Downloading https://files.pythonhosted.org/packages/06/8c/3003a41d5229e65da792331b060dcad8100a0a5b9760f8c2074cde864148/nnfs-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqMQ5-2NMyGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d151818-4b86-4a6d-c26e-b0711134fe0e"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "output = np.maximum(0, inputs)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tk8KBVaaCyW"
      },
      "source": [
        "# Relu Activation Class \n",
        "class Activation_Relu:\n",
        "\n",
        "  #forward pass \n",
        "  def forward(self, inputs):\n",
        "    # calculate max of 0, input values\n",
        "    self.output = np.maximum(0, inputs)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d394Le7caPPW",
        "outputId": "7e8ea4e6-fbe5-4d89-a99c-adce87656ccd"
      },
      "source": [
        "#example \n",
        "import numpy as np \n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "#Dense Layer\n",
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    #initialize weight and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1,n_neurons))\n",
        "\n",
        "  # forwards pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# test class\n",
        "\n",
        "X,y = spiral_data(samples = 100, classes=3)\n",
        "\n",
        "#create the layer\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "activation1 = Activation_Relu()\n",
        "\n",
        "#perform a forward pass\n",
        "dense1.forward(X)\n",
        "\n",
        "#perform activation of Relu\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "\n",
        "# see output\n",
        "print(activation1.output[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.        ]\n",
            " [0.         0.00011395 0.        ]\n",
            " [0.         0.00031729 0.        ]\n",
            " [0.         0.00052666 0.        ]\n",
            " [0.         0.00071401 0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btpa74zdasnS"
      },
      "source": [
        "We can see the values have been clipped to 0. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xFMcq_OcId2"
      },
      "source": [
        "### Softmax Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bPWQbQmaTg3",
        "outputId": "2ce77170-e208-4f67-c938-1ef00e70a5d8"
      },
      "source": [
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "\n",
        "# need to do e^x\n",
        "# e = 2.71828182846\n",
        "\n",
        "exp_values = np.exp(layer_outputs)\n",
        "print(exp_values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[121.51041752   3.35348465  10.85906266]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgIvJ3nKcpp6"
      },
      "source": [
        "Full softmax "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XHV_VVmcmHN",
        "outputId": "c317597e-db2b-4726-9916-ae358e6a1087"
      },
      "source": [
        "layer_outputs = [4.8, 1.21, 2.385]\n",
        "\n",
        "# need to do e^x\n",
        "# e = 2.71828182846\n",
        "\n",
        "exp_values = np.exp(layer_outputs)\n",
        "print(exp_values)\n",
        "\n",
        "# now normalize them \n",
        "norm_values = exp_values / np.sum(exp_values)\n",
        "print(norm_values)\n",
        "\n",
        "print(\"sum of normalized values: \", np.sum(norm_values))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[121.51041752   3.35348465  10.85906266]\n",
            "[0.89528266 0.02470831 0.08000903]\n",
            "sum of normalized values:  0.9999999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxpCWfxCdpmk"
      },
      "source": [
        "For batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgb8mqBxdlkg",
        "outputId": "06543410-60d0-4197-d825-8c35f39f3999"
      },
      "source": [
        "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
        "                          [8.9, -1.81, 0.2],\n",
        "                          [1.41, 1.051, 0.026]])\n",
        "\n",
        "print(\"sum without axis\", np.sum(layer_outputs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sum without axis 18.172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKk-RKUWee9p",
        "outputId": "2a2819f5-a8fd-4b65-e069-46627df7903c"
      },
      "source": [
        "print(\"sum with axis = 0 \", np.sum(layer_outputs, axis = 0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sum with axis = 0  [15.11   0.451  2.611]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF_6qDQ3ekrP",
        "outputId": "880ca3a1-5839-433a-d139-75445f678349"
      },
      "source": [
        "print(\"sum with axis = 1 \", np.sum(layer_outputs, axis = 1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sum with axis = 1  [8.395 7.29  2.487]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465dw3D4enpZ",
        "outputId": "304c5967-534a-45f4-9ec3-9788116623ff"
      },
      "source": [
        "# to simplify to single value per sample, use keep_dims\n",
        "print(\"sum with axis = 1 \", np.sum(layer_outputs, axis = 1, keepdims=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sum with axis = 1  [[8.395]\n",
            " [7.29 ]\n",
            " [2.487]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpW-1vffGAC"
      },
      "source": [
        "\n",
        "\n",
        "Softmax Activation Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2mchQCQfCcQ"
      },
      "source": [
        "#Softmax Activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "  # forward pass \n",
        "  def forward(self, inputs):\n",
        "\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims=True))\n",
        "\n",
        "    #normalize\n",
        "    probs = exp_values / np.sum(exp_values, axis = 1, keepdims=True)\n",
        "\n",
        "    self.output = probs\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QblO6R7EfjC_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aAE0BP-fxrP"
      },
      "source": [
        "Exponent\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4khkxb3Hfy44",
        "outputId": "2dbde7e9-057c-46de-db48-252f86df8467"
      },
      "source": [
        "np.exp(-100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.720075976020836e-44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leILyMxqf0Lq",
        "outputId": "cfdc566f-425f-4e0e-b02b-8e17fefd1ad5"
      },
      "source": [
        "np.exp(-10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5399929762484854e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzFmyWO9f1T1",
        "outputId": "14a98d10-daa3-46ed-cbc3-51e9fa0b53c1"
      },
      "source": [
        "np.exp(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ic3tchhf2Mc",
        "outputId": "50580099-14d7-42bd-f62a-911c730e9ece"
      },
      "source": [
        "np.exp(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22026.465794806718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuWQqSmDf6Sh",
        "outputId": "57927eb2-76bb-48ad-a0d1-d2e273d854b9"
      },
      "source": [
        "np.exp(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.6881171418161356e+43"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD9GrnCJf9np",
        "outputId": "98ef0188-e014-43e4-cf35-fba4511ef37e"
      },
      "source": [
        "np.exp(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hPSob2mf-l4",
        "outputId": "65afaede-773e-4915-ec14-80f714be2313"
      },
      "source": [
        "softmax = Activation_Softmax()\n",
        "\n",
        "input = [[1,2,3]]\n",
        "\n",
        "softmax.forward(input)\n",
        "print(softmax.output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.09003057 0.24472847 0.66524096]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veP3ehuhggKP",
        "outputId": "c73b67b5-6584-44e8-b75f-bf72ada139e7"
      },
      "source": [
        "input = [[-2,-1,0]]\n",
        "\n",
        "softmax.forward(input)\n",
        "print(softmax.output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.09003057 0.24472847 0.66524096]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygnayfuHgzKY"
      },
      "source": [
        "Final Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mn-Quxqgryh",
        "outputId": "7aac4615-f633-4525-91d1-ebfd704489cb"
      },
      "source": [
        "#example \n",
        "import numpy as np \n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "#Dense Layer\n",
        "class Layer_Dense:\n",
        "\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    #initialize weight and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1,n_neurons))\n",
        "\n",
        "  # forwards pass\n",
        "  def forward(self, inputs):\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# test class\n",
        "\n",
        "X,y = spiral_data(samples = 100, classes=3)\n",
        "\n",
        "#create the layer\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "activation1 = Activation_Relu()\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "#perform a forward pass\n",
        "dense1.forward(X)\n",
        "\n",
        "#perform activation of Relu\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "#perform softmax\n",
        "activation2.forward(activation1.output)\n",
        "\n",
        "# see output\n",
        "print(activation2.output[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33332068 0.33335868 0.33332068]\n",
            " [0.3332981  0.33340386 0.3332981 ]\n",
            " [0.3332748  0.3334504  0.3332748 ]\n",
            " [0.33325398 0.33349204 0.33325398]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJoRO_LRiBk0"
      },
      "source": [
        "# assignment \n",
        "Think about how to model sine wave using neural networks?\n",
        "\n",
        "Can a neural network do this? We have not learned about training but think about the first steps. \n",
        "\n",
        "You need a dataset to train upon. How can you create that dataset?\n",
        "\n",
        "Try to create a sine wave dataset. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8ibRs5fytDO"
      },
      "source": [
        "Categorical Cross Entropy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_41MFWHqhGWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1164f1-5398-41ce-efe3-1d1c66dd687e"
      },
      "source": [
        "import math\n",
        "output = [0.7, 0.1 , 0.2]\n",
        "\n",
        "target = [1, 0, 0 ]\n",
        "\n",
        "\n",
        "loss = -(math.log(output[0]) * target[0] + \n",
        "                 math.log(output[1]) * target[1]+\n",
        "                 math.log(output[2]) * target[2])\n",
        "\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.35667494393873245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eks2bKtdy9t2",
        "outputId": "1517063f-3974-4faf-9334-8ecd14a194c1"
      },
      "source": [
        "# our numpy solution \n",
        "import numpy as np \n",
        "np.sum(-1*np.log(np.asarray(output)) * np.asarray(target))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35667494393873245"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WxtkqzOzPfg",
        "outputId": "42ee921d-f837-4c40-c9fc-a8f988f37ee6"
      },
      "source": [
        "print(math.log(1))\n",
        "print(math.log(0.95))\n",
        "print(math.log(0.9))\n",
        "print(math.log(0.8))\n",
        "print(math.log(0.2))\n",
        "print(math.log(0.1))\n",
        "print(math.log(0.05))\n",
        "print(math.log(0.01))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "-0.05129329438755058\n",
            "-0.10536051565782628\n",
            "-0.2231435513142097\n",
            "-1.6094379124341003\n",
            "-2.3025850929940455\n",
            "-2.995732273553991\n",
            "-4.605170185988091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez9A2iiq0Rc5"
      },
      "source": [
        "Calcualting loss "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VqGAkbyzfhn",
        "outputId": "34cadc1c-6c92-4c6a-92c8-df00fae1ccd8"
      },
      "source": [
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
        "                   [0.1, 0.5, 0.4],\n",
        "                   [0.02, 0.9, 0.08]])\n",
        "\n",
        "class_targets = [0, 1, 1]\n",
        "\n",
        "#use numpy indexing to get the correct confidence scores \n",
        "print(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7 0.5 0.9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijj8XFQq03CA",
        "outputId": "8b7587ba-0e6d-404c-986b-e1957e6f0804"
      },
      "source": [
        "negLog = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
        "\n",
        "avg_loss = np.mean(negLog)\n",
        "\n",
        "print(avg_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.38506088005216804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3shsHer21e5d"
      },
      "source": [
        "## One hot encoded values. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X85R8wQM1Zqf",
        "outputId": "170b2f83-951f-4c03-f491-033aa78c26ce"
      },
      "source": [
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
        "                   [0.1, 0.5, 0.4],\n",
        "                   [0.02, 0.9, 0.08]])\n",
        "\n",
        "class_targets = np.array([[1, 0, 0],\n",
        "                          [0,1,0],\n",
        "                         [0,1,0]])\n",
        "\n",
        "print(np.sum(softmax_outputs * class_targets, axis = 1))\n",
        "negLog = -1 * np.log(np.sum(softmax_outputs * class_targets, axis = 1))\n",
        "avg_loss = np.mean(negLog)\n",
        "print(avg_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7 0.5 0.9]\n",
            "0.38506088005216804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN-P7pwk4bP-"
      },
      "source": [
        "Problems with log "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1pS8Nqf37Pt",
        "outputId": "fd56f29c-6f7c-4413-a515-ce8e8f14f364"
      },
      "source": [
        "np.log(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-inf"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLJERz_B4dvH",
        "outputId": "33b8541f-1b0b-422e-dadf-ce06b1ee0514"
      },
      "source": [
        "-np.log(1.0000001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-9.999999505838704e-08"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efdQ4NEu4pBR"
      },
      "source": [
        "add a very small value to the actual values like 1e-7\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zYItRv54gEO"
      },
      "source": [
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
        "                   [0.1, 0.5, 0.4],\n",
        "                   [0.02, 0.9, 0.08]])\n",
        "\n",
        "class_targets = np.array([[1, 0, 0],\n",
        "                          [1,0,0],\n",
        "                         [0,1,0]])\n",
        "\n",
        "actual_scores = np.sum(softmax_outputs * class_targets, axis = 1)\n",
        "\n",
        "actual_scores += 1e-7\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yVGvz2X48Kw",
        "outputId": "4f6fe102-6b6a-4c39-f309-53547789d97f"
      },
      "source": [
        "print(actual_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7000001 0.5000001 0.9000001]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n9T9WUi4-Ww",
        "outputId": "1df98f6f-c040-4adf-9379-cd92fbe1df74"
      },
      "source": [
        "preds = np.array([0, 1, 0.5])\n",
        "preds += 1e-7\n",
        "\n",
        "preds = np.clip(preds, 1e-7, 1-1e-7)\n",
        "np.clip()\n",
        "print(preds)\n",
        "\n",
        "-np.log(preds)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.000000e-07 9.999999e-01 5.000001e-01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.61180957e+01, 1.00000005e-07, 6.93146981e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTnSPanL5zeN"
      },
      "source": [
        "Final Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWvL0gd75b8I"
      },
      "source": [
        "# Common loss class \n",
        "\n",
        "class Loss : \n",
        "\n",
        "  #calculates data and reg loss given model output and truth values \n",
        "  def calculate(self, output, y):\n",
        "\n",
        "    #calculate sample losses \n",
        "\n",
        "    sample_losses = self.forward(output, y )\n",
        "\n",
        "    #mean loss \n",
        "    data_loss = np.mean(sample_losses)\n",
        "\n",
        "    #return \n",
        "    return data_loss \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEAeMjaU6MCo"
      },
      "source": [
        "# Categorical Cross Entropy Loss \n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "\n",
        "  #forward pass \n",
        "  def forward(self, y_pred, y_true):\n",
        "\n",
        "    #number of samples in a batch \n",
        "    samples = len(y_pred)\n",
        "\n",
        "    #clip data to remove log 0 and negative loss \n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
        "\n",
        "    #probs for target values \n",
        "\n",
        "    #if categorical labels \n",
        "    if len(y_true.shape) == 1:\n",
        "      confidences = y_pred_clipped[range(samples), y_true]\n",
        "    \n",
        "    elif len(y_true.shape) == 2:\n",
        "      confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
        "\n",
        "    #Losses\n",
        "    negLog = -np.log(confidences)\n",
        "\n",
        "    return negLog\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr2L3C1z6bHC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fqzbqw6l6SQv",
        "outputId": "ac11ca76-9f56-45a9-f36b-6099ce8ec822"
      },
      "source": [
        "loss_fucntion = Loss_CategoricalCrossEntropy()\n",
        "loss = loss_fucntion.calculate(softmax_outputs, class_targets)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.38506088005216804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAVuiWKd7Tsk"
      },
      "source": [
        "Calculating Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM7ySp-57MYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58dd27a2-a1cc-4636-b2c2-c64bf5ad07b4"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "#probabilities of 3 samples \n",
        "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
        "                            [0.5, 0.1, 0.4],\n",
        "                            [0.02, 0.9, 0.08]])\n",
        "\n",
        "#target \n",
        "class_targets = np.array([0, 1, 1])\n",
        "\n",
        "\n",
        "predictions = np.argmax(softmax_outputs, axis = 1)\n",
        "\n",
        "accuracy = np.mean(predictions == class_targets)\n",
        "\n",
        "print(accuracy)\n",
        "\n",
        "#"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FChidiTQFcK5"
      },
      "source": [
        "Back Propagation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7MBENBb_pEE",
        "outputId": "d7a1901b-c885-4b21-d666-9acb45f5d397"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "# Suppose gradient is coming from the end. \n",
        "dvalues = np.array([[1., 1., 1.],\n",
        "                    [2., 2., 2.],\n",
        "                    3., 3., 3.])\n",
        "\n",
        "\n",
        "# set of inputs \n",
        "inputs = np.array([[1, 2, 3, 2.5],\n",
        "                   [2., 5., -1,  2],\n",
        "                   [-1.5, 2.7, 3.3, -0.8]])\n",
        "\n",
        "#weights \n",
        "\n",
        "weights = np.array( [[ 0.2, 0.8, -0.5, 1],\n",
        "                     [0.5, -0.91, 0.26, -0.5],\n",
        "                     [-0.26, -0.27, 0.17, 0.87]]).T\n",
        "\n",
        "biases = np.array([[2, 3, 0.5]])\n",
        "\n",
        "#forward pass \n",
        "layer_outputs = np.dot(inputs, weights) + biases \n",
        "relu_outputs = np.maximum(0, layer_outputs)\n",
        "\n",
        "#backward pass \n",
        "#relu layer\n",
        "drelu = relu_outputs.copy()\n",
        "drelu[layer_outputs <= 0] = 0\n",
        "\n",
        "#dense layer\n",
        "dinputs = np.dot(drelu, weights.T)\n",
        "dweights = np.dot(inputs.T, drelu)\n",
        "dbiases = np.sum(drelu, axis = 0, keepdims = True)\n",
        "\n",
        "weights += -0.001 * dweights \n",
        "biases += -0.001 * dbiases \n",
        "\n",
        "print(weights)\n",
        "print(biases )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.179515   0.5003665 -0.262746 ]\n",
            " [ 0.742093  -0.9152577 -0.2758402]\n",
            " [-0.510153   0.2529017  0.1629592]\n",
            " [ 0.971328  -0.5021842  0.8636583]]\n",
            "[[1.98489  2.997739 0.497389]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFH0YjCMcArU"
      },
      "source": [
        "## Full NN code without Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39QnLCvUcLm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa3653f-9f5c-41d3-978f-c9242080a56a"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnfs\n",
            "  Downloading https://files.pythonhosted.org/packages/06/8c/3003a41d5229e65da792331b060dcad8100a0a5b9760f8c2074cde864148/nnfs-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQgUKbExHAbD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7509ed1-274e-421e-fc1f-9bc3c9efcba0"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "\n",
        "# ReLU activation\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 3)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Perform a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Perform a forward pass through activation function\n",
        "# takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Perform a forward pass through second Dense layer\n",
        "# takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Perform a forward pass through the activation/loss function\n",
        "# takes the output of second dense layer here and returns loss\n",
        "loss = loss_activation.forward(dense2.output, y)\n",
        "# Let's see output of the first few samples:\n",
        "print(loss_activation.output[:5])\n",
        "\n",
        "# Print loss value\n",
        "print('loss:', loss)\n",
        "\n",
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along first axis\n",
        "predictions = np.argmax(loss_activation.output, axis=1)\n",
        "if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis=1)\n",
        "accuracy = np.mean(predictions==y)\n",
        "\n",
        "# Print accuracy\n",
        "print('acc:', accuracy)\n",
        "\n",
        "# Backward pass\n",
        "loss_activation.backward(loss_activation.output, y)\n",
        "dense2.backward(loss_activation.dinputs)\n",
        "activation1.backward(dense2.dinputs)\n",
        "dense1.backward(activation1.dinputs)\n",
        "\n",
        "# Print gradients\n",
        "print(dense1.dweights)\n",
        "print(dense1.dbiases)\n",
        "print(dense2.dweights)\n",
        "print(dense2.dbiases)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.3333332  0.3333332  0.33333364]\n",
            " [0.3333329  0.33333293 0.3333342 ]\n",
            " [0.3333326  0.33333263 0.33333477]\n",
            " [0.33333233 0.3333324  0.33333528]]\n",
            "loss: 1.0986104\n",
            "acc: 0.34\n",
            "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
            " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
            "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
            "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
            " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
            " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
            "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oQgRSO0Gf5F"
      },
      "source": [
        "## Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPDdjKMA_cib"
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OERkShhI_o8p"
      },
      "source": [
        "class Optimizer_SGD:\n",
        "\n",
        "  def __init__(self, learning_rate = 1.0):\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def update_params(self, layer):\n",
        "    layer.weights += -self.learning_rate * layer.dweights\n",
        "    layer.biases += -self.learning_rate * layer.dbiases\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY_yXGMc_XfW",
        "outputId": "2499c84f-ebd9-4269-ae11-33ed33ef5fe4"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_SGD()\n",
        "\n",
        "#train in loop \n",
        "losses = []\n",
        "for epoch in range(10001):\n",
        "\n",
        "  #forward passes \n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "  losses.append(loss)\n",
        "  # calcualte accuracy \n",
        "  predictions = np.argmax(loss_activation.output, axis = 1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis = 1)\n",
        "  \n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, acc: {accuracy}, loss:{loss}' )\n",
        "\n",
        "  # backward pass \n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  #update params \n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.36, loss:1.098594307899475\n",
            "epoch: 100, acc: 0.4, loss:1.0869206190109253\n",
            "epoch: 200, acc: 0.4166666666666667, loss:1.0773380994796753\n",
            "epoch: 300, acc: 0.42, loss:1.0760209560394287\n",
            "epoch: 400, acc: 0.4, loss:1.074236273765564\n",
            "epoch: 500, acc: 0.4, loss:1.0713080167770386\n",
            "epoch: 600, acc: 0.4166666666666667, loss:1.0673632621765137\n",
            "epoch: 700, acc: 0.43666666666666665, loss:1.0623551607131958\n",
            "epoch: 800, acc: 0.43, loss:1.0550928115844727\n",
            "epoch: 900, acc: 0.39, loss:1.0638744831085205\n",
            "epoch: 1000, acc: 0.4, loss:1.0622682571411133\n",
            "epoch: 1100, acc: 0.44333333333333336, loss:1.061262607574463\n",
            "epoch: 1200, acc: 0.4033333333333333, loss:1.0605604648590088\n",
            "epoch: 1300, acc: 0.38666666666666666, loss:1.0517452955245972\n",
            "epoch: 1400, acc: 0.38666666666666666, loss:1.105523943901062\n",
            "epoch: 1500, acc: 0.43, loss:1.0429925918579102\n",
            "epoch: 1600, acc: 0.41, loss:1.0630847215652466\n",
            "epoch: 1700, acc: 0.39666666666666667, loss:1.0433818101882935\n",
            "epoch: 1800, acc: 0.45, loss:1.0381176471710205\n",
            "epoch: 1900, acc: 0.48333333333333334, loss:1.0248881578445435\n",
            "epoch: 2000, acc: 0.4033333333333333, loss:1.0371609926223755\n",
            "epoch: 2100, acc: 0.45666666666666667, loss:1.0216883420944214\n",
            "epoch: 2200, acc: 0.49333333333333335, loss:1.0195891857147217\n",
            "epoch: 2300, acc: 0.44333333333333336, loss:1.0016921758651733\n",
            "epoch: 2400, acc: 0.48, loss:0.9941015839576721\n",
            "epoch: 2500, acc: 0.49, loss:1.008725643157959\n",
            "epoch: 2600, acc: 0.48, loss:0.9909303784370422\n",
            "epoch: 2700, acc: 0.54, loss:0.9724604487419128\n",
            "epoch: 2800, acc: 0.47333333333333333, loss:0.9976078271865845\n",
            "epoch: 2900, acc: 0.5266666666666666, loss:0.963260293006897\n",
            "epoch: 3000, acc: 0.5433333333333333, loss:0.9854065179824829\n",
            "epoch: 3100, acc: 0.5233333333333333, loss:0.9876247048377991\n",
            "epoch: 3200, acc: 0.4866666666666667, loss:0.9763368964195251\n",
            "epoch: 3300, acc: 0.49333333333333335, loss:0.9725401997566223\n",
            "epoch: 3400, acc: 0.4766666666666667, loss:0.9842291474342346\n",
            "epoch: 3500, acc: 0.52, loss:0.992677628993988\n",
            "epoch: 3600, acc: 0.5366666666666666, loss:0.9652496576309204\n",
            "epoch: 3700, acc: 0.55, loss:0.9966951608657837\n",
            "epoch: 3800, acc: 0.48333333333333334, loss:0.9633467793464661\n",
            "epoch: 3900, acc: 0.49333333333333335, loss:0.9697713255882263\n",
            "epoch: 4000, acc: 0.52, loss:0.9706616401672363\n",
            "epoch: 4100, acc: 0.54, loss:0.989279568195343\n",
            "epoch: 4200, acc: 0.5466666666666666, loss:0.9564687013626099\n",
            "epoch: 4300, acc: 0.5733333333333334, loss:0.9951723217964172\n",
            "epoch: 4400, acc: 0.48333333333333334, loss:0.9586665034294128\n",
            "epoch: 4500, acc: 0.49333333333333335, loss:0.9687966704368591\n",
            "epoch: 4600, acc: 0.52, loss:0.9737345576286316\n",
            "epoch: 4700, acc: 0.5433333333333333, loss:0.9868637323379517\n",
            "epoch: 4800, acc: 0.5466666666666666, loss:0.9624319672584534\n",
            "epoch: 4900, acc: 0.5533333333333333, loss:1.0066593885421753\n",
            "epoch: 5000, acc: 0.51, loss:0.9731101393699646\n",
            "epoch: 5100, acc: 0.53, loss:0.9664764404296875\n",
            "epoch: 5200, acc: 0.5466666666666666, loss:0.9857028126716614\n",
            "epoch: 5300, acc: 0.55, loss:0.9605168700218201\n",
            "epoch: 5400, acc: 0.5566666666666666, loss:1.0007402896881104\n",
            "epoch: 5500, acc: 0.52, loss:0.9658766388893127\n",
            "epoch: 5600, acc: 0.5133333333333333, loss:0.9555531740188599\n",
            "epoch: 5700, acc: 0.57, loss:0.9754876494407654\n",
            "epoch: 5800, acc: 0.5533333333333333, loss:0.9360553026199341\n",
            "epoch: 5900, acc: 0.5766666666666667, loss:0.971545398235321\n",
            "epoch: 6000, acc: 0.49, loss:0.9397631883621216\n",
            "epoch: 6100, acc: 0.5566666666666666, loss:0.9508057832717896\n",
            "epoch: 6200, acc: 0.5333333333333333, loss:0.9367452263832092\n",
            "epoch: 6300, acc: 0.59, loss:0.9558148384094238\n",
            "epoch: 6400, acc: 0.5633333333333334, loss:0.928519606590271\n",
            "epoch: 6500, acc: 0.5933333333333334, loss:0.9553053975105286\n",
            "epoch: 6600, acc: 0.5233333333333333, loss:0.9191462993621826\n",
            "epoch: 6700, acc: 0.5666666666666667, loss:0.928459882736206\n",
            "epoch: 6800, acc: 0.56, loss:0.8899131417274475\n",
            "epoch: 6900, acc: 0.5866666666666667, loss:0.8712476491928101\n",
            "epoch: 7000, acc: 0.5933333333333334, loss:0.9049868583679199\n",
            "epoch: 7100, acc: 0.6233333333333333, loss:0.859682023525238\n",
            "epoch: 7200, acc: 0.5833333333333334, loss:0.867811918258667\n",
            "epoch: 7300, acc: 0.6, loss:0.8740476369857788\n",
            "epoch: 7400, acc: 0.5966666666666667, loss:0.8488327860832214\n",
            "epoch: 7500, acc: 0.6066666666666667, loss:0.91400146484375\n",
            "epoch: 7600, acc: 0.6566666666666666, loss:0.8581396341323853\n",
            "epoch: 7700, acc: 0.62, loss:0.8528146743774414\n",
            "epoch: 7800, acc: 0.63, loss:0.8621673583984375\n",
            "epoch: 7900, acc: 0.58, loss:0.8781864643096924\n",
            "epoch: 8000, acc: 0.6166666666666667, loss:0.8736241459846497\n",
            "epoch: 8100, acc: 0.5966666666666667, loss:0.8394070267677307\n",
            "epoch: 8200, acc: 0.5933333333333334, loss:0.8517529964447021\n",
            "epoch: 8300, acc: 0.58, loss:0.9232891798019409\n",
            "epoch: 8400, acc: 0.6466666666666666, loss:0.8787612915039062\n",
            "epoch: 8500, acc: 0.63, loss:0.8380792140960693\n",
            "epoch: 8600, acc: 0.63, loss:0.8532760739326477\n",
            "epoch: 8700, acc: 0.6266666666666667, loss:0.8633965253829956\n",
            "epoch: 8800, acc: 0.6, loss:0.8790981769561768\n",
            "epoch: 8900, acc: 0.5966666666666667, loss:0.8709477782249451\n",
            "epoch: 9000, acc: 0.57, loss:0.8717381954193115\n",
            "epoch: 9100, acc: 0.63, loss:0.8798957467079163\n",
            "epoch: 9200, acc: 0.6266666666666667, loss:0.8619816303253174\n",
            "epoch: 9300, acc: 0.6133333333333333, loss:0.8477188348770142\n",
            "epoch: 9400, acc: 0.5966666666666667, loss:0.8380523920059204\n",
            "epoch: 9500, acc: 0.6066666666666667, loss:0.8435631394386292\n",
            "epoch: 9600, acc: 0.6066666666666667, loss:0.8644261956214905\n",
            "epoch: 9700, acc: 0.6066666666666667, loss:0.8806635737419128\n",
            "epoch: 9800, acc: 0.6, loss:0.9264081716537476\n",
            "epoch: 9900, acc: 0.61, loss:0.9145286083221436\n",
            "epoch: 10000, acc: 0.6466666666666666, loss:0.8737250566482544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "Xvgha_rMBr-y",
        "outputId": "97d7c736-9070-46e8-983a-d31109047ca4"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline \n",
        "\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title('Loss vs steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUW8MMB5CHDf"
      },
      "source": [
        "Low learning rate \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1KSlZfJxCGoW",
        "outputId": "7f83173e-152b-422c-c72d-344ea440539e"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_SGD(0.001)\n",
        "\n",
        "#train in loop \n",
        "losses = []\n",
        "for epoch in range(10001):\n",
        "\n",
        "  #forward passes \n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "  losses.append(loss)\n",
        "  # calcualte accuracy \n",
        "  predictions = np.argmax(loss_activation.output, axis = 1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis = 1)\n",
        "  \n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, acc: {accuracy}, loss:{loss}' )\n",
        "\n",
        "  # backward pass \n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  #update params \n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title('Loss vs steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.36, loss:1.098594307899475\n",
            "epoch: 100, acc: 0.36, loss:1.0985922813415527\n",
            "epoch: 200, acc: 0.36666666666666664, loss:1.0985902547836304\n",
            "epoch: 300, acc: 0.37, loss:1.0985881090164185\n",
            "epoch: 400, acc: 0.4, loss:1.098586082458496\n",
            "epoch: 500, acc: 0.4066666666666667, loss:1.0985839366912842\n",
            "epoch: 600, acc: 0.4033333333333333, loss:1.0985819101333618\n",
            "epoch: 700, acc: 0.4033333333333333, loss:1.0985798835754395\n",
            "epoch: 800, acc: 0.39666666666666667, loss:1.0985779762268066\n",
            "epoch: 900, acc: 0.4033333333333333, loss:1.0985760688781738\n",
            "epoch: 1000, acc: 0.4066666666666667, loss:1.0985740423202515\n",
            "epoch: 1100, acc: 0.4033333333333333, loss:1.0985721349716187\n",
            "epoch: 1200, acc: 0.4066666666666667, loss:1.0985701084136963\n",
            "epoch: 1300, acc: 0.4033333333333333, loss:1.098568320274353\n",
            "epoch: 1400, acc: 0.39666666666666667, loss:1.0985665321350098\n",
            "epoch: 1500, acc: 0.3933333333333333, loss:1.0985647439956665\n",
            "epoch: 1600, acc: 0.39, loss:1.0985629558563232\n",
            "epoch: 1700, acc: 0.39, loss:1.0985610485076904\n",
            "epoch: 1800, acc: 0.38666666666666666, loss:1.0985593795776367\n",
            "epoch: 1900, acc: 0.38666666666666666, loss:1.0985575914382935\n",
            "epoch: 2000, acc: 0.39, loss:1.0985559225082397\n",
            "epoch: 2100, acc: 0.38666666666666666, loss:1.098554253578186\n",
            "epoch: 2200, acc: 0.39, loss:1.0985527038574219\n",
            "epoch: 2300, acc: 0.39, loss:1.0985511541366577\n",
            "epoch: 2400, acc: 0.39, loss:1.098549485206604\n",
            "epoch: 2500, acc: 0.3933333333333333, loss:1.0985479354858398\n",
            "epoch: 2600, acc: 0.3933333333333333, loss:1.0985465049743652\n",
            "epoch: 2700, acc: 0.39666666666666667, loss:1.098544955253601\n",
            "epoch: 2800, acc: 0.4, loss:1.0985435247421265\n",
            "epoch: 2900, acc: 0.4, loss:1.0985420942306519\n",
            "epoch: 3000, acc: 0.4, loss:1.0985406637191772\n",
            "epoch: 3100, acc: 0.4, loss:1.0985392332077026\n",
            "epoch: 3200, acc: 0.4033333333333333, loss:1.098537802696228\n",
            "epoch: 3300, acc: 0.4, loss:1.098536491394043\n",
            "epoch: 3400, acc: 0.4033333333333333, loss:1.0985350608825684\n",
            "epoch: 3500, acc: 0.4066666666666667, loss:1.0985337495803833\n",
            "epoch: 3600, acc: 0.41, loss:1.0985324382781982\n",
            "epoch: 3700, acc: 0.41333333333333333, loss:1.0985311269760132\n",
            "epoch: 3800, acc: 0.41333333333333333, loss:1.0985296964645386\n",
            "epoch: 3900, acc: 0.41333333333333333, loss:1.0985283851623535\n",
            "epoch: 4000, acc: 0.4166666666666667, loss:1.0985270738601685\n",
            "epoch: 4100, acc: 0.4166666666666667, loss:1.0985256433486938\n",
            "epoch: 4200, acc: 0.4166666666666667, loss:1.0985243320465088\n",
            "epoch: 4300, acc: 0.4166666666666667, loss:1.0985230207443237\n",
            "epoch: 4400, acc: 0.4166666666666667, loss:1.0985217094421387\n",
            "epoch: 4500, acc: 0.4166666666666667, loss:1.0985205173492432\n",
            "epoch: 4600, acc: 0.4166666666666667, loss:1.0985193252563477\n",
            "epoch: 4700, acc: 0.4166666666666667, loss:1.0985180139541626\n",
            "epoch: 4800, acc: 0.4166666666666667, loss:1.0985167026519775\n",
            "epoch: 4900, acc: 0.4166666666666667, loss:1.0985153913497925\n",
            "epoch: 5000, acc: 0.4166666666666667, loss:1.098514199256897\n",
            "epoch: 5100, acc: 0.4166666666666667, loss:1.0985130071640015\n",
            "epoch: 5200, acc: 0.4166666666666667, loss:1.098511815071106\n",
            "epoch: 5300, acc: 0.4166666666666667, loss:1.0985103845596313\n",
            "epoch: 5400, acc: 0.4166666666666667, loss:1.0985091924667358\n",
            "epoch: 5500, acc: 0.4166666666666667, loss:1.0985080003738403\n",
            "epoch: 5600, acc: 0.41333333333333333, loss:1.0985068082809448\n",
            "epoch: 5700, acc: 0.41333333333333333, loss:1.0985054969787598\n",
            "epoch: 5800, acc: 0.41333333333333333, loss:1.0985041856765747\n",
            "epoch: 5900, acc: 0.41333333333333333, loss:1.0985028743743896\n",
            "epoch: 6000, acc: 0.41, loss:1.0985018014907837\n",
            "epoch: 6100, acc: 0.41, loss:1.0985004901885986\n",
            "epoch: 6200, acc: 0.4066666666666667, loss:1.0984992980957031\n",
            "epoch: 6300, acc: 0.4066666666666667, loss:1.0984981060028076\n",
            "epoch: 6400, acc: 0.4066666666666667, loss:1.098496675491333\n",
            "epoch: 6500, acc: 0.41, loss:1.0984957218170166\n",
            "epoch: 6600, acc: 0.41, loss:1.0984944105148315\n",
            "epoch: 6700, acc: 0.41, loss:1.0984930992126465\n",
            "epoch: 6800, acc: 0.41, loss:1.098491907119751\n",
            "epoch: 6900, acc: 0.41, loss:1.098490595817566\n",
            "epoch: 7000, acc: 0.41, loss:1.0984896421432495\n",
            "epoch: 7100, acc: 0.41, loss:1.098488211631775\n",
            "epoch: 7200, acc: 0.41333333333333333, loss:1.0984869003295898\n",
            "epoch: 7300, acc: 0.41, loss:1.0984857082366943\n",
            "epoch: 7400, acc: 0.41, loss:1.0984845161437988\n",
            "epoch: 7500, acc: 0.41, loss:1.0984833240509033\n",
            "epoch: 7600, acc: 0.41, loss:1.0984820127487183\n",
            "epoch: 7700, acc: 0.41, loss:1.0984808206558228\n",
            "epoch: 7800, acc: 0.41, loss:1.0984796285629272\n",
            "epoch: 7900, acc: 0.41, loss:1.0984784364700317\n",
            "epoch: 8000, acc: 0.41, loss:1.0984771251678467\n",
            "epoch: 8100, acc: 0.41, loss:1.0984759330749512\n",
            "epoch: 8200, acc: 0.41, loss:1.0984746217727661\n",
            "epoch: 8300, acc: 0.41333333333333333, loss:1.0984734296798706\n",
            "epoch: 8400, acc: 0.41333333333333333, loss:1.0984721183776855\n",
            "epoch: 8500, acc: 0.41333333333333333, loss:1.09847092628479\n",
            "epoch: 8600, acc: 0.41333333333333333, loss:1.098469614982605\n",
            "epoch: 8700, acc: 0.41333333333333333, loss:1.0984684228897095\n",
            "epoch: 8800, acc: 0.41333333333333333, loss:1.098467230796814\n",
            "epoch: 8900, acc: 0.41333333333333333, loss:1.0984660387039185\n",
            "epoch: 9000, acc: 0.41333333333333333, loss:1.0984646081924438\n",
            "epoch: 9100, acc: 0.4166666666666667, loss:1.0984632968902588\n",
            "epoch: 9200, acc: 0.42, loss:1.0984621047973633\n",
            "epoch: 9300, acc: 0.42, loss:1.0984609127044678\n",
            "epoch: 9400, acc: 0.42, loss:1.0984594821929932\n",
            "epoch: 9500, acc: 0.42, loss:1.0984582901000977\n",
            "epoch: 9600, acc: 0.42, loss:1.0984569787979126\n",
            "epoch: 9700, acc: 0.42, loss:1.098455548286438\n",
            "epoch: 9800, acc: 0.42, loss:1.098454236984253\n",
            "epoch: 9900, acc: 0.42, loss:1.0984528064727783\n",
            "epoch: 10000, acc: 0.42333333333333334, loss:1.0984514951705933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93K8PgMVDEN0"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_SGD(0.85)\n",
        "\n",
        "#train in loop \n",
        "losses = []\n",
        "for epoch in range(10001):\n",
        "\n",
        "  #forward passes \n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "  losses.append(loss)\n",
        "  # calcualte accuracy \n",
        "  predictions = np.argmax(loss_activation.output, axis = 1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis = 1)\n",
        "  \n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, acc: {accuracy}, loss:{loss}' )\n",
        "\n",
        "  # backward pass \n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  #update params \n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title('Loss vs steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luAkdVEc8Yy1"
      },
      "source": [
        "### SGD Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SilHYmJpDWc6"
      },
      "source": [
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "        # Vanilla SGD updates \n",
        "        weight_updates = -self.current_learning_rate * \\\n",
        "                          layer.dweights\n",
        "        bias_updates = -self.current_learning_rate * \\\n",
        "                        layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6aA7A1cDmd2"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_SGD(decay = 1e-2)\n",
        "\n",
        "#train in loop \n",
        "losses = []\n",
        "for epoch in range(10001):\n",
        "\n",
        "  #forward passes \n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "  losses.append(loss)\n",
        "  # calcualte accuracy \n",
        "  predictions = np.argmax(loss_activation.output, axis = 1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis = 1)\n",
        "  \n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, acc: {accuracy}, loss:{loss}, lr:{optimizer.current_learning_rate}' )\n",
        "\n",
        "  # backward pass \n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  #update params \n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()\n",
        "\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title('Loss vs steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHnoAADQEWCI"
      },
      "source": [
        "Less decay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVttgor2EXqy"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_SGD(decay = 1e-3)\n",
        "\n",
        "#train in loop \n",
        "losses = []\n",
        "for epoch in range(10001):\n",
        "\n",
        "  #forward passes \n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "  losses.append(loss)\n",
        "  # calcualte accuracy \n",
        "  predictions = np.argmax(loss_activation.output, axis = 1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis = 1)\n",
        "  \n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, acc: {accuracy}, loss:{loss}, lr:{optimizer.current_learning_rate}' )\n",
        "\n",
        "  # backward pass \n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  #update params \n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()\n",
        "\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title('Loss vs steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw9UwNdMN8Il"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_SGD(learning_rate = 0.9, decay = 1e-3)\n",
        "\n",
        "#train in loop \n",
        "losses = []\n",
        "for epoch in range(10001):\n",
        "\n",
        "  #forward passes \n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "  losses.append(loss)\n",
        "  # calcualte accuracy \n",
        "  predictions = np.argmax(loss_activation.output, axis = 1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis = 1)\n",
        "  \n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, acc: {accuracy}, loss:{loss}, lr:{optimizer.current_learning_rate}' )\n",
        "\n",
        "  # backward pass \n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  #update params \n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()\n",
        "\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title('Loss vs steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEx4hZ_l8XnG"
      },
      "source": [
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Mn4VOiCdpP"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_SGD(decay = 1e-3, momentum  =0.5)\n",
        "\n",
        "#train in loop \n",
        "losses = []\n",
        "for epoch in range(25000):\n",
        "\n",
        "  #forward passes \n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "  losses.append(loss)\n",
        "  # calcualte accuracy \n",
        "  predictions = np.argmax(loss_activation.output, axis = 1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis = 1)\n",
        "  \n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, acc: {accuracy}, loss:{loss}, lr:{optimizer.current_learning_rate}' )\n",
        "\n",
        "  # backward pass \n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  #update params \n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()\n",
        "\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title('Loss vs steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_yPvhER-wd7"
      },
      "source": [
        "## Optimizer Adagrad\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltvH9Bx5-znr"
      },
      "source": [
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights**2\n",
        "        layer.bias_cache += layer.dbiases**2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlUBcMqP_LM9",
        "outputId": "80551278-4a10-4af6-f32f-2fca4ec371f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install nnfs"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nnfs\n",
            "  Downloading https://files.pythonhosted.org/packages/06/8c/3003a41d5229e65da792331b060dcad8100a0a5b9760f8c2074cde864148/nnfs-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.19.5)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "8pWMYqkg-44f",
        "outputId": "e12d0022-c802-4521-8f1d-f1a82cbffd49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 3 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_Adagrad(decay = 1e-4)\n",
        "\n",
        "#train in loop \n",
        "losses = []\n",
        "for epoch in range(10001):\n",
        "\n",
        "  #forward passes \n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "  dense2.forward(activation1.output)\n",
        "  loss = loss_activation.forward(dense2.output, y)\n",
        "  losses.append(loss)\n",
        "  # calcualte accuracy \n",
        "  predictions = np.argmax(loss_activation.output, axis = 1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis = 1)\n",
        "  \n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, acc: {accuracy}, loss:{loss}, lr:{optimizer.current_learning_rate}' )\n",
        "\n",
        "  # backward pass \n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  #update params \n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()\n",
        "\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.title('Loss vs steps')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-153f93a71986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create Dense layer with 2 input features and 3 output values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdense1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayer_Dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Create ReLU activation (to be used with Dense layer):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Layer_Dense' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gpGygrucG0J"
      },
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "\n",
        "\n",
        "# Dense layer\n",
        "class Layer_Dense:\n",
        "\n",
        "    # Layer initialization\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Initialize weights and biases\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs, weights and biases\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Gradients on parameters\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        # Gradient on values\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "\n",
        "# ReLU activation\n",
        "class Activation_ReLU:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "        # Calculate output values from inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Since we need to modify original variable,\n",
        "        # let's make a copy of values first\n",
        "        self.dinputs = dvalues.copy()\n",
        "\n",
        "        # Zero gradient where input values were negative\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "\n",
        "\n",
        "# Softmax activation\n",
        "class Activation_Softmax:\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs):\n",
        "        # Remember input values\n",
        "        self.inputs = inputs\n",
        "\n",
        "        # Get unnormalized probabilities\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
        "                                            keepdims=True))\n",
        "        # Normalize them for each sample\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
        "                                            keepdims=True)\n",
        "\n",
        "        self.output = probabilities\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in \\\n",
        "                enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output\n",
        "            jacobian_matrix = np.diagflat(single_output) - \\\n",
        "                              np.dot(single_output, single_output.T)\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
        "                                         single_dvalues)\n",
        "\n",
        "\n",
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "    # Initialize optimizer - set settings,\n",
        "    # learning rate of 1. is default for this optimizer\n",
        "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.momentum = momentum\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If we use momentum\n",
        "        if self.momentum:\n",
        "\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            # filled with zeros\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                # If there is no momentum array for weights\n",
        "                # The array doesn't exist for biases yet either.\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # Build weight updates with momentum - take previous\n",
        "            # updates multiplied by retain factor and update with\n",
        "            # current gradients\n",
        "            weight_updates = \\\n",
        "                self.momentum * layer.weight_momentums - \\\n",
        "                self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            # Build bias updates\n",
        "            bias_updates = \\\n",
        "                self.momentum * layer.bias_momentums - \\\n",
        "                self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        # Vanilla SGD updates (as before momentum update)\n",
        "        else:\n",
        "            weight_updates = -self.current_learning_rate * \\\n",
        "                             layer.dweights\n",
        "            bias_updates = -self.current_learning_rate * \\\n",
        "                           layer.dbiases\n",
        "\n",
        "        # Update weights and biases using either\n",
        "        # vanilla or momentum updates\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache += layer.dweights**2\n",
        "        layer.bias_cache += layer.dbiases**2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 rho=0.9):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.rho = rho\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "            (1 - self.rho) * layer.dweights**2\n",
        "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "            (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         layer.dweights / \\\n",
        "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                        layer.dbiases / \\\n",
        "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "\n",
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "\n",
        "    # Initialize optimizer - set settings\n",
        "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
        "                 beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # Call once before any parameter updates\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            self.current_learning_rate = self.learning_rate * \\\n",
        "                (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    # Update parameters\n",
        "    def update_params(self, layer):\n",
        "\n",
        "        # If layer does not contain cache arrays,\n",
        "        # create them filled with zeros\n",
        "        if not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Update momentum  with current gradients\n",
        "        layer.weight_momentums = self.beta_1 * \\\n",
        "                                 layer.weight_momentums + \\\n",
        "                                 (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * \\\n",
        "                               layer.bias_momentums + \\\n",
        "                               (1 - self.beta_1) * layer.dbiases\n",
        "        # Get corrected momentum\n",
        "        # self.iteration is 0 at first pass\n",
        "        # and we need to start with 1 here\n",
        "        weight_momentums_corrected = layer.weight_momentums / \\\n",
        "            (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / \\\n",
        "            (1 - self.beta_1 ** (self.iterations + 1))\n",
        "        # Update cache with squared current gradients\n",
        "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
        "            (1 - self.beta_2) * layer.dweights**2\n",
        "\n",
        "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
        "            (1 - self.beta_2) * layer.dbiases**2\n",
        "        # Get corrected cache\n",
        "        weight_cache_corrected = layer.weight_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_cache_corrected = layer.bias_cache / \\\n",
        "            (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "        # Vanilla SGD parameter update + normalization\n",
        "        # with square rooted cache\n",
        "        layer.weights += -self.current_learning_rate * \\\n",
        "                         weight_momentums_corrected / \\\n",
        "                         (np.sqrt(weight_cache_corrected) +\n",
        "                             self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * \\\n",
        "                         bias_momentums_corrected / \\\n",
        "                         (np.sqrt(bias_cache_corrected) +\n",
        "                             self.epsilon)\n",
        "\n",
        "    # Call once after any parameter updates\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1\n",
        "\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "    # Calculates the data and regularization losses\n",
        "    # given model output and ground truth values\n",
        "    def calculate(self, output, y):\n",
        "\n",
        "        # Calculate sample losses\n",
        "        sample_losses = self.forward(output, y)\n",
        "\n",
        "        # Calculate mean loss\n",
        "        data_loss = np.mean(sample_losses)\n",
        "\n",
        "        # Return loss\n",
        "        return data_loss\n",
        "\n",
        "\n",
        "# Cross-entropy loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, y_pred, y_true):\n",
        "\n",
        "        # Number of samples in a batch\n",
        "        samples = len(y_pred)\n",
        "        # Clip data to prevent division by 0\n",
        "        # Clip both sides to not drag mean towards any value\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for target values -\n",
        "        # only if categorical labels\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[\n",
        "                range(samples),\n",
        "                y_true\n",
        "            ]\n",
        "\n",
        "        # Mask values - only for one-hot encoded labels\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(\n",
        "                y_pred_clipped * y_true,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        # Calculate gradient\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "\n",
        "# Softmax classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "\n",
        "    # Creates activation and loss function objects\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    # Forward pass\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "    # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        # If labels are one-hot encoded,\n",
        "        # turn them into discrete values\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "dense1 = Layer_Dense(2, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output\n",
        "# of previous layer here) and 3 output values (output values)\n",
        "dense2 = Layer_Dense(64, 3)\n",
        "# Create Softmax classifier's combined loss and activation\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "\n",
        "    # Perform a forward pass of our training data through this layer\n",
        "    dense1.forward(X)\n",
        "\n",
        "    # Perform a forward pass through activation function\n",
        "    # takes the output of first dense layer here\n",
        "    activation1.forward(dense1.output)\n",
        "\n",
        "    # Perform a forward pass through second Dense layer\n",
        "    # takes outputs of activation function of first layer as inputs\n",
        "    dense2.forward(activation1.output)\n",
        "\n",
        "    # Perform a forward pass through the activation/loss function\n",
        "    # takes the output of second dense layer here and returns loss\n",
        "    loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "    # Calculate accuracy from output of activation2 and targets\n",
        "    # calculate values along first axis\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y.shape) == 2:\n",
        "        y = np.argmax(y, axis=1)\n",
        "    accuracy = np.mean(predictions==y)\n",
        "\n",
        "    if not epoch % 100:\n",
        "        print(f'epoch: {epoch}, ' +\n",
        "              f'acc: {accuracy:.3f}, ' +\n",
        "              f'loss: {loss:.3f}, ' +\n",
        "              f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Update weights and biases\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}